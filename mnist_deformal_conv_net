#!/usr/bin/env python
# -*- coding: utf-8 -*-
# author：albert date:2020/4/30 time:11:36

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import time
from torchvision import transforms
from torch.utils.data import DataLoader
from torchvision.datasets import mnist
from torch.autograd import Variable
from torch import optim


###构建可变性卷积 适用于非刚体，学习其空间几何形变
class DeformConv2D(nn.Module):
    def __init__(self,in_channel,out_channel,kernel_size=3,padding=2,bias=None):
        super(DeformConv2D,self).__init__()
        self.kernel_size=kernel_size
        self.padding=padding
        self.zero_padding=nn.ZeroPad2d(padding)
        self.conv_kernel=nn.Conv2d(in_channel,out_channel,kernel_size=kernel_size,stride=kernel_size,bias=bias)

    def forward(self,x,offset):
        dtype=offset.data.type()
        ks=self.kernel_size

        N=offset.size(1)//2
        offset_index=torch.autograd.Variable(torch.cat([torch.arange(0,2*N,2),torch.arange(1,2*N+1,2)]),requires_grad=False).type_as(x).long()
        offset_index=offset_index.unsqueeze(dim=0).unsqueeze(dim=-1).unsqueeze(dim=-1).expand(*offset.size())
        offset=torch.gather(offset,dim=1,index=offset_index)

        if self.padding:
            x=self.zero_padding(x)

        p=self._get_p(offset,dtype)
        p=p.contiguous().permute(0,2,3,1)
        q_lt=torch.autograd.Variable(p.data,requires_grad=False).floor()
        q_rb=q_lt+1

        q_lt = torch.cat([torch.clamp(q_lt[..., :N], 0, x.size(2)-1), torch.clamp(q_lt[..., N:], 0, x.size(3)-1)], dim=-1).long()
        q_rb = torch.cat([torch.clamp(q_rb[..., :N], 0, x.size(2)-1), torch.clamp(q_rb[..., N:], 0, x.size(3)-1)], dim=-1).long()
        q_lb = torch.cat([q_lt[..., :N], q_rb[..., N:]], -1)
        q_rt = torch.cat([q_rb[..., :N], q_lt[..., N:]], -1)

        mask=torch.cat([p[..., :N].lt(self.padding)+p[..., :N].gt(x.size(2)-1-self.padding),
                          p[..., N:].lt(self.padding)+p[..., N:].gt(x.size(3)-1-self.padding)], dim=-1).type_as(p)
        mask=mask.detach()

        floor_p=p-(p-torch.floor(p))
        p=p*(1-mask)+floor_p*mask
        p=torch.cat([torch.clamp(p[...,:N],0,x.size(2)-1),torch.clamp(p[...,N:],0,x.size(3)-1)],dim=-1)

        g_lt = (1 + (q_lt[..., :N].type_as(p) - p[..., :N])) * (1 + (q_lt[..., N:].type_as(p) - p[..., N:]))
        g_rb = (1 - (q_rb[..., :N].type_as(p) - p[..., :N])) * (1 - (q_rb[..., N:].type_as(p) - p[..., N:]))
        g_lb = (1 + (q_lb[..., :N].type_as(p) - p[..., :N])) * (1 - (q_lb[..., N:].type_as(p) - p[..., N:]))
        g_rt = (1 - (q_rt[..., :N].type_as(p) - p[..., :N])) * (1 + (q_rt[..., N:].type_as(p) - p[..., N:]))

        x_q_lt=self._get_x_q(x,q_lt,N)
        x_q_rb=self._get_x_q(x,q_rb,N)
        x_q_lb=self._get_x_q(x,q_lb,N)
        x_q_rt=self._get_x_q(x,q_rt,N)

        x_offset=g_lt.unsqueeze(dim=1)*x_q_lt\
                 +g_rb.unsqueeze(dim=1)+x_q_rb\
                 +g_lb.unsqueeze(dim=1)*x_q_lb\
                 +g_rt.unsqueeze(dim=1)*x_q_rt

        x_offset=self._reshape_x_offset(x_offset,ks)
        out=self.conv_kernel(x_offset)

        return out


    def _get_p_n(self,N,dtype):
        p_n_x,p_n_y=np.meshgrid(range(-(self.kernel_size-1)//2,(self.kernel_size-1)//2+1),
                                range(-(self.kernel_size-1)//2,(self.kernel_size-1)//2+1),indexing="ij")
        p_n=np.concatenate((p_n_x.flatten(),p_n_y.flatten()))
        p_n=np.reshape(p_n,(1,2*N,1,1))
        p_n=torch.autograd.Variable(torch.from_numpy(p_n).type(dtype),requires_grad=False)
        return p_n

    @staticmethod
    def _get_p_0(h, w, N, dtype):
        p_0_x, p_0_y = np.meshgrid(range(1, h+1), range(1, w+1), indexing='ij')
        p_0_x = p_0_x.flatten().reshape(1, 1, h, w).repeat(N, axis=1)
        p_0_y = p_0_y.flatten().reshape(1, 1, h, w).repeat(N, axis=1)
        p_0 = np.concatenate((p_0_x, p_0_y), axis=1)
        p_0 = torch.autograd.Variable(torch.from_numpy(p_0).type(dtype), requires_grad=False)
        return p_0

    def _get_p(self, offset, dtype):
        N, h, w = offset.size(1)//2, offset.size(2), offset.size(3)
        # (1, 2N, 1, 1)
        p_n = self._get_p_n(N, dtype)
        #print("p_n: {}".format(p_n.shape))
        # (1, 2N, h, w)
        p_0 = self._get_p_0(h, w, N, dtype)
        p = p_0 + p_n
        #print(p)
        return p + offset

    def _get_x_q(self,x,q,N):
        b,h,w,_=q.size()
        padded_w=x.size(3)
        c=x.size(1)
        x=x.contiguous().view(b,c,-1)
        index=q[...,:N]*padded_w+q[...,N:]
        index=index.contiguous().unsqueeze(dim=1).expand(-1,c,-1,-1,-1).contiguous().view(b,c,-1)
        x_offset=x.gather(dim=-1,index=index).contiguous().view(b,c,h,w,N)
        return x_offset

    @staticmethod
    def _reshape_x_offset(x_offset,ks):
        b,c,h,w,N=x_offset.size()
        x_offset=torch.cat([x_offset[...,s:s+ks].contiguous().view(b,c,h,w*ks) for s in range(0,N,ks)],dim=-1)
        x_offset=x_offset.contiguous().view(b,c,h*ks,w*ks)
        return x_offset

class DeformableConv(nn.Module):
    def __init__(self):
        super(DeformableConv,self).__init__()
        self.maxpool=nn.MaxPool2d((2,2),stride=2)
        self.conv1=nn.Conv2d(1,32,kernel_size=3,padding=1)
        self.bn1=nn.BatchNorm2d(32)

        self.conv2=nn.Conv2d(32,64,kernel_size=3,padding=1)
        self.bn2=nn.BatchNorm2d(64)

        self.conv3=nn.Conv2d(64,128,kernel_size=3,padding=1)
        self.bn3=nn.BatchNorm2d(128)

        self.offsets=nn.Conv2d(128,18,kernel_size=3,padding=1)
        self.conv4=DeformConv2D(128,128,kernel_size=3,padding=1)  ##增加变形卷积
        self.bn4=nn.BatchNorm2d(128)
        self.classfier=nn.Linear(128,10)

    def forward(self,x):
        #import pdb;pdb.set_trace()
        x=F.relu(self.conv1(x))
        x=self.bn1(x)
        x=self.maxpool(x)
        x=F.relu(self.conv2(x))
        x=self.bn2(x)
        x=self.maxpool(x)
        x=F.relu(self.conv3(x))
        #x=self.maxpool(x)
        offsets=self.offsets(x)
        x=F.relu(self.conv4(x,offsets))
        x=self.bn4(x)
        x=F.avg_pool2d(x,kernel_size=7,stride=1).view(x.size(0),-1)
        x=self.classfier(x)
        return F.log_softmax(x,dim=1)


####读取数据
def get_data():
    # train_set=mnist.MNIST('./data',train=True)
    # test_set=mnist.MNIST('./data',train=False)

    data_tf=transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.5],[0.5])
    ])

    train_set=mnist.MNIST('./data',train=True,transform=data_tf,download=True)
    test_set=mnist.MNIST('./data',train=False,transform=data_tf,download=True)

    train_data=DataLoader(train_set,batch_size=64,shuffle=True)
    test_data=DataLoader(test_set,batch_size=128,shuffle=False)

    return train_data,test_data

def train():
    train_iter,test_iter=get_data()
    num_classes=10
    num_epoches=5

    model=DeformableConv()
    loss=nn.CrossEntropyLoss()
    optimizer=optim.SGD(model.parameters(),lr=1e-1)

    for epoch in range(num_epoches):
        train_loss,train_acc=0,0
        for cnt,(img,label) in enumerate(train_iter):
            #  img,label=Variable(img),Variable(label)
            #import pdb;pdb.set_trace()
            out_logits=model(img)
            cal_loss=loss(out_logits,label)

            optimizer.zero_grad()
            cal_loss.backward()
            optimizer.step()

            train_loss+=cal_loss.item()
            _,pred=out_logits.max(1)
            train_acc=(pred==label).sum().item()/img.shape[0]
            print("train epoch:%d,train loss:%f,train_acc:%f"%(epoch,cal_loss.item(),train_acc))

        eval_loss,eval_acc=0,0
        for img,label in test_iter:
            #import pdb;pdb.set_trace()
            #img,label=Variable(img),Variable(label)

            out_=model(img)
            loss_=loss(out_,label)

            eval_loss+=loss_
            _,pred=out_.max(1)
            eval_acc+=(pred==label).sum().item()/img.shape[0]
        print("test epoch:%d,test_loss:%f,test_acc:%f"%(epoch,eval_loss/len(test_iter),eval_acc/len(test_iter)))

if __name__=="__main__":
    train()
