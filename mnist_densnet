import torch
from torch import nn
from collections import OrderedDict
import torchvision.models
from torch.utils.data import DataLoader
from torchvision.datasets import mnist
from torchvision import transforms
from torch.autograd import Variable
from torch import optim


class _DenseLayer(nn.Sequential):
    def __init__(self,in_channels,growth_rate,bn_size,drop_rate):
        super(_DenseLayer,self).__init__()
        self.add_module('norml',nn.BatchNorm2d(in_channels))
        self.add_module('relu1',nn.ReLU(inplace=True))
        self.add_module('conv1',nn.Conv2d(in_channels,bn_size*growth_rate,kernel_size=1,stride=1,bias=False))
        self.add_module('norm2',nn.BatchNorm2d(bn_size*growth_rate))
        self.add_module('relu2',nn.ReLU(inplace=True))
        self.add_module('conv2',nn.Conv2d(bn_size*growth_rate,growth_rate,kernel_size=3,stride=1,padding=1,bias=False))
        self.drop_rate=drop_rate

    def forward(self,x):
        new_features=super(_DenseLayer,self).forward(x)
        if self.drop_rate >0:
            new_features=nn.functional.dropout(new_features,p=self.drop_rate,training=self.training)
        return torch.cat([x,new_features],1)

class _DenseBlock(nn.Sequential):
    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):
        super(_DenseBlock, self).__init__()
        for i in range(num_layers):
            layer = _DenseLayer(num_input_features + i * growth_rate, growth_rate, bn_size, drop_rate)
            self.add_module('denselayer%d' % (i + 1), layer)

class _Transition(nn.Sequential):
    def __init__(self,in_channels,out_channels):
        super(_Transition,self).__init__()
        self.add_module('norm',nn.BatchNorm2d(in_channels))
        self.add_module('relu',nn.ReLU(inplace=True))
        self.add_module('conv',nn.Conv2d(in_channels,out_channels,kernel_size=1,stride=1,bias=False))
        self.add_module('pool',nn.AvgPool2d(kernel_size=2,stride=2))


class DenseNet(nn.Module):
    r"""Densenet-BC model class, based on
    `"Densely Connected Convolutional Networks" <https://arxiv.org/pdf/1608.06993.pdf>`_
    Args:
        growth_rate (int) - how many filters to add each layer (`k` in paper)
        block_config (list of 4 ints) - how many layers in each pooling block
        num_init_features (int) - the number of filters to learn in the first convolution layer
        bn_size (int) - multiplicative factor for number of bottle neck layers
          (i.e. bn_size * k features in the bottleneck layer)
        drop_rate (float) - dropout rate after each dense layer
        num_classes (int) - number of classification classes
    """

    def __init__(self, growth_rate=12, block_config=(12, 12, 12),
                 num_init_features=24, bn_size=4, drop_rate=0.2, num_classes=10):

        super(DenseNet, self).__init__()

        # First convolution
        self.features = nn.Sequential(OrderedDict([
            ('conv0', nn.Conv2d(1, num_init_features, kernel_size=3, stride=1, padding=1, bias=False)),
            ('norm0', nn.BatchNorm2d(num_init_features)),
            ('relu0', nn.ReLU(inplace=True)),
        ]))

        # Each denseblock
        num_features = num_init_features
        for i, num_layers in enumerate(block_config):
            block = _DenseBlock(num_layers=num_layers, num_input_features=num_features,bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate)
            self.features.add_module('denseblock%d' % (i + 1), block)
            num_features = num_features + num_layers * growth_rate
            if i != len(block_config) - 1:
                trans = _Transition(in_channels=num_features, out_channels=num_features // 2)
                self.features.add_module('transition%d' % (i + 1), trans)
                num_features = num_features // 2

        # Final batch norm
        self.features.add_module('norm5', nn.BatchNorm2d(num_features))

        # Linear layer
        self.classifier = nn.Linear(num_features, num_classes)

        # Official init from torch repo.
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        #import pdb;pdb.set_trace()
        x=nn.functional.interpolate(x,(32,32))
        features = self.features(x)
        out =nn.functional.relu(features, inplace=True)
        out = nn.functional.avg_pool2d(out, kernel_size=8, stride=1)
        out=out.view(features.size(0), -1)
        out = self.classifier(out)
        return out

####读取数据
def get_data():
    # train_set=mnist.MNIST('./data',train=True)
    # test_set=mnist.MNIST('./data',train=False)

    data_tf=transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.5],[0.5])
    ])

    train_set=mnist.MNIST('./data',train=True,transform=data_tf,download=True)
    test_set=mnist.MNIST('./data',train=False,transform=data_tf,download=True)

    train_data=DataLoader(train_set,batch_size=64,shuffle=True)
    test_data=DataLoader(test_set,batch_size=64,shuffle=False)

    return train_data,test_data

def train():
    train_iter,test_iter=get_data()
    num_classes=10
    num_epoches=5

    ###导入模型，确定运行环境
    model=DenseNet()
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    model.to(device)

    loss=nn.CrossEntropyLoss()
    optimizer=optim.SGD(model.parameters(),lr=1e-2)

    for epoch in range(num_epoches):
        train_loss,train_acc=0,0
        for cnt,(img,label) in enumerate(train_iter):
            #  img,label=Variable(img),Variable(label)
            #import pdb;pdb.set_trace()
            img,label=img.to(device),label.to(device)
            out_logits=model(img)
            cal_loss=loss(out_logits,label)

            optimizer.zero_grad()
            cal_loss.backward()
            optimizer.step()

            train_loss+=cal_loss.item()
            _,pred=out_logits.max(1)
            train_acc=(pred==label).sum().item()/img.shape[0]
            print("train epoch:%d,train loss:%f,train_acc:%f"%(epoch,cal_loss.item(),train_acc))

        eval_loss,eval_acc=0,0
        for img,label in test_iter:
            #import pdb;pdb.set_trace()
            #img,label=Variable(img),Variable(label)

            out_=model(img)
            loss_=loss(out_,label)

            eval_loss+=loss_
            _,pred=out_.max(1)
            eval_acc+=(pred==label).sum().item()/img.shape[0]
        print("test epoch:%d,test_loss:%f,test_acc:%f"%(epoch,eval_loss/len(test_iter),eval_acc/len(test_iter)))

if __name__=="__main__":
    train()
