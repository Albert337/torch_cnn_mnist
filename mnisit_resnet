#!/usr/bin/env python
# -*- coding: utf-8 -*-
# author：albert time:2020/4/28

import torch
import os
import math
from torch import nn
import torchvision.models
from torch.utils.data import DataLoader
from torchvision.datasets import mnist
from torchvision import transforms
from torch.autograd import Variable
from torch import optim


class BasicBlock(nn.Module):
    expansion = 1
    def __init__(self,in_channels,out_channels,stride=1,downsample=None):
        super(BasicBlock,self).__init__()
        self.conv1=nn.Conv2d(in_channels,out_channels,kernel_size=3,stride=stride,padding=1)
        self.bn1=nn.BatchNorm2d(out_channels)
        self.relu=nn.ReLU(inplace=True)
        self.conv2=nn.Conv2d(out_channels,out_channels,kernel_size=3,stride=1,padding=1)
        self.bn2=nn.BatchNorm2d(out_channels)
        self.downsample=downsample
        self.stride=stride

    def forward(self,x):
        residual=x
        out=self.conv1(x)
        out=self.bn1(out)
        out=self.relu(out)

        out=self.conv2(out)
        out=self.bn2(out)
        if self.downsample is not None:
            residual=self.downsample(x)

        out+=residual
        out=self.relu(out)

        return out

class Bottleneck(nn.Module):
    expansion=4
    def __init__(self,in_channels,out_channels,stride=1,downsample=None):
        super(Bottleneck,self).__init__()
        self.conv1=nn.Conv2d(in_channels,out_channels,kernel_size=1,stride=1,padding=0)
        self.bn1=nn.BatchNorm2d(out_channels)
        self.conv2=nn.Conv2d(out_channels,out_channels,kernel_size=3,stride=stride,padding=1,bias=False)
        self.bn2=nn.BatchNorm2d(out_channels)
        self.conv3=nn.Conv2d(out_channels,out_channels*4,kernel_size=1,stride=1,padding=0,bias=False)
        self.bn3=nn.BatchNorm2d(out_channels*4)
        self.relu=nn.ReLU(inplace=True)
        self.downsample=downsample
        self.stride=stride

    def forward(self,x):
        residual=x

        out=self.conv1(x)
        out=self.bn1(out)
        out=self.relu(out)

        out=self.conv2(out)
        out=self.bn2(out)
        out=self.relu(out)

        out=self.conv3(out)
        out=self.bn3(out)

        if self.downsample is not None:
            residual=self.downsample(x)

        out+=residual
        out=self.relu(out)

        return out


class ResNet(nn.Module):
    def __init__(self,block,layers,num_classes=10):
        self.inplanes=64
        super(ResNet,self).__init__()
        self.conv1=nn.Conv2d(1,64,kernel_size=7,stride=2,padding=3,bias=False)
        self.bn1=nn.BatchNorm2d(64)
        self.relu=nn.ReLU(inplace=True)
        self.layer1=self._make_layer(block,64,layers[0])
        self.layer2=self._make_layer(block,128,layers[1],stride=2)
        self.layer3=self._make_layer(block,256,layers[2],stride=2)
        self.layer4=self._make_layer(block,512,layers[3],stride=2)
        self.avgpool=nn.AvgPool2d(14)  ##这是根据224输入尺寸计算得到，可以跟据输入大小进行修改
        self.fc=nn.Linear(512*block.expansion,num_classes)


        ###手动对网络层权重进行初始化，这样有利于加速收敛，适用于模型复杂度高
        ###详细可参考https://discuss.pytorch.org/t/pytorch-self-module/49677/4
        for m in self.modules():   ###为继承的父类属性
            if isinstance(m,nn.Conv2d):
                n=m.kernel_size[0]*m.kernel_size[1]*m.out_channels
                m.weight.data.normal_(0,math.sqrt(2./n))
            elif isinstance(m,nn.BatchNorm2d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()

    def _make_layer(self,block,planes,blocks,stride=1):
        downsample=None
        if stride!=1 or self.inplanes!=planes*block.expansion:
            downsample=nn.Sequential(
                nn.Conv2d(self.inplanes,planes*block.expansion,kernel_size=1,stride=stride,bias=False),
                nn.BatchNorm2d(planes*block.expansion),
            )

        layers=[]
        layers.append(block(self.inplanes,planes,stride,downsample))
        self.inplanes=planes*block.expansion
        for i in range(1,blocks):
            layers.append(block(self.inplanes,planes))

        return nn.Sequential(*layers)

    def forward(self,x):
        #import pdb;pdb.set_trace()
        x=nn.functional.interpolate(x,(224,224))
        x=self.conv1(x)
        x=self.bn1(x)
        x=self.relu(x)
        x=self.layer1(x)
        x=self.layer2(x)
        x=self.layer3(x)
        x=self.layer4(x)
        x=self.avgpool(x)
        x=x.view(x.size(0),-1)
        x=self.fc(x)
        return x


def resnet18(**kwargs):
    """Constructs a ResNet-18 model.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
    """
    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)
    return model


def resnet34(**kwargs):
    """Constructs a ResNet-34 model.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
    """
    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)
    return model


def resnet50(**kwargs):
    """Constructs a ResNet-50 model.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
    """
    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)
    return model


def resnet101(**kwargs):
    """Constructs a ResNet-101 model.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
    """
    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)
    return model


def resnet152(**kwargs):
    """Constructs a ResNet-152 model.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
    """
    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)
    return model

####读取数据
def get_data():
    # train_set=mnist.MNIST('./data',train=True)
    # test_set=mnist.MNIST('./data',train=False)

    data_tf=transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.5],[0.5])
    ])

    train_set=mnist.MNIST('./data',train=True,transform=data_tf,download=True)
    test_set=mnist.MNIST('./data',train=False,transform=data_tf,download=True)

    train_data=DataLoader(train_set,batch_size=64,shuffle=True)
    test_data=DataLoader(test_set,batch_size=128,shuffle=False)

    return train_data,test_data

def train():
    train_iter,test_iter=get_data()
    num_classes=10
    num_epoches=5

    model=resnet18()
    loss=nn.CrossEntropyLoss()
    optimizer=optim.SGD(model.parameters(),lr=1e-1)

    for epoch in range(num_epoches):
        train_loss,train_acc=0,0
        for cnt,(img,label) in enumerate(train_iter):
            #img,label=Variable(img),Variable(label)
            #import pdb;pdb.set_trace()
            out_logits=model(img)
            cal_loss=loss(out_logits,label)

            optimizer.zero_grad()
            cal_loss.backward()
            optimizer.step()

            train_loss+=cal_loss.item()
            _,pred=out_logits.max(1)
            train_acc=(pred==label).sum().item()/img.shape[0]
            print("train epoch:%d,train loss:%f,train_acc:%f"%(epoch,cal_loss.item(),train_acc))

        eval_loss,eval_acc=0,0
        for img,label in test_iter:
            #import pdb;pdb.set_trace()
            #img,label=Variable(img),Variable(label)

            out_=model(img)
            loss_=loss(out_,label)

            eval_loss+=loss_
            _,pred=out_.max(1)
            eval_acc+=(pred==label).sum().item()/img.shape[0]
        print("test epoch:%d,test_loss:%f,test_acc:%f"%(epoch,eval_loss/len(test_iter),eval_acc/len(test_iter)))


if __name__=="__main__":
    train()
