#!/usr/bin/env python
# -*- coding: utf-8 -*-
# author：albert date:2020/4/30 time:11:22

import torch
import os
from torch import nn
from torch.utils.data import DataLoader
from torchvision.datasets import mnist
from torchvision import transforms
from torch.autograd import Variable
from torch import optim

class Inception(nn.Module):
    def __init__(self,in_planes,n,pool_planes):
        super(Inception,self).__init__()
        n1,n2,n3,n4,n5=n
        self.branch1=nn.Sequential(
            nn.Conv2d(in_planes,n1,kernel_size=1)
        )

        self.branch2=nn.Sequential(
            nn.Conv2d(in_planes,n2,kernel_size=1,stride=1,padding=0),
            nn.Conv2d(n2,n3,kernel_size=3,stride=1,padding=1)
        )

        self.branch3=nn.Sequential(
            nn.Conv2d(in_planes,n4,kernel_size=1,stride=1,padding=1),
            nn.Conv2d(n4,n5,kernel_size=5,stride=1,padding=1)
        )

        self.branch4=nn.Sequential(
            nn.MaxPool2d(kernel_size=3,stride=1),
            nn.Conv2d(in_planes,pool_planes,kernel_size=1,padding=1)
        )


    def forward(self,x):
        #import pdb;pdb.set_trace()
        x1=self.branch1(x)
        x2=self.branch2(x)
        x3=self.branch3(x)
        x4=self.branch4(x)
        x=torch.cat([x1,x2,x3,x4],axis=1)
        return x

class GoogleNet(nn.Module):
    def __init__(self):
        super(GoogleNet,self).__init__()

        self.conv1=nn.Sequential(
            nn.Conv2d(1,64,kernel_size=7,stride=2,padding=3),  ###卷积除不尽的情况向下取整
            nn.MaxPool2d(kernel_size=3,stride=2,padding=1)     ###池化除不尽向下取整
        )

        self.conv2=nn.Sequential(
            nn.Conv2d(64,192,kernel_size=3,stride=1,padding=1),
            nn.MaxPool2d(kernel_size=3,stride=2,padding=1)
        )

        self.inception_3=nn.Sequential(
            Inception(192,[64,96,128,16,32],32),
            Inception(256,[128,128,192,32,96],64),
            nn.MaxPool2d(kernel_size=3,stride=2,padding=1)
        )

        self.inception_4=nn.Sequential(
            Inception(480,[192,96,208,16,48],64),
            Inception(512,[160,112,224,24,64],64),
            Inception(512,[128,128,256,24,64],64),
            Inception(512,[112,144,288,32,64],64),
            Inception(528,[256,160,320,32,128],128),
            nn.MaxPool2d(kernel_size=3,stride=2,padding=1)
        )

        self.inception_5=nn.Sequential(
            Inception(832,[256,160,320,32,128],128),
            Inception(832,[384,192,384,48,128],128)
        )

        self.avg_pool=nn.AvgPool2d(kernel_size=7,stride=1)
        self.dropout=nn.Dropout(0.4)
        self.linear=nn.Linear(1024,10)  ####最后的输出根据样本来定

    def forward(self,x):
        #import pdb;pdb.set_trace()
        x=nn.functional.interpolate(x,(224,224))  ##batch_size*1*224*224
        x=self.conv1(x)    ##batch_size*64*56*56
        x=self.conv2(x)
        x=self.inception_3(x)
        x=self.inception_4(x)
        x=self.inception_5(x)
        x=self.avg_pool(x)
        x=torch.squeeze(x,dim=-1)
        x=torch.squeeze(x,dim=-1)
        x=self.dropout(x)
        x=self.linear(x)
        return x

def get_data():
    data_tf = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.5], [0.5])
    ])

    train_set = mnist.MNIST('./data', train=True, transform=data_tf, download=True)
    test_set = mnist.MNIST('./data', train=False, transform=data_tf, download=True)

    train_data = DataLoader(train_set, batch_size=64, shuffle=True)
    test_data = DataLoader(test_set, batch_size=128, shuffle=False)

    return train_data, test_data


def train():
    train_iter, test_iter = get_data()
    num_classes = 10
    num_epoches = 5

    model = GoogleNet()
    loss = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=1e-1)

    for epoch in range(num_epoches):
        train_loss, train_acc = 0, 0
        for cnt, (img, label) in enumerate(train_iter):
            #  img,label=Variable(img),Variable(label)
            # import pdb;pdb.set_trace()
            out_logits = model(img)
            cal_loss = loss(out_logits, label)

            optimizer.zero_grad()
            cal_loss.backward()
            optimizer.step()

            train_loss += cal_loss.item()
            _, pred = out_logits.max(1)
            train_acc = (pred == label).sum().item() / img.shape[0]
            print("train epoch:%d,train loss:%f,train_acc:%f" % (epoch, cal_loss.item(), train_acc))

        eval_loss, eval_acc = 0, 0
        for img, label in test_iter:
            # import pdb;pdb.set_trace()
            # img,label=Variable(img),Variable(label)

            out_ = model(img)
            loss_ = loss(out_, label)

            eval_loss += loss_
            _, pred = out_.max(1)
            eval_acc += (pred == label).sum().item() / img.shape[0]
        print("test epoch:%d,test_loss:%f,test_acc:%f" % (epoch, eval_loss / len(test_iter), eval_acc / len(test_iter)))


if __name__=="__main__":
    train()


